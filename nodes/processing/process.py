# Copyright (c) 2025 Andrea Pozzetti
# SPDX-License-Identifier: MIT
"""
Processing node for SAM 3D Body.

Performs 3D human mesh reconstruction from a single image.
"""

import os
import glob
import tempfile
import torch
import numpy as np
import cv2
import folder_paths
from ..base import comfy_image_to_numpy, comfy_mask_to_numpy


def find_mhr_model_path(mesh_data=None):
    """Find the MHR model path using multiple fallback strategies."""
    # Strategy 1: Check mesh_data for explicitly provided path
    if mesh_data and mesh_data.get("mhr_path"):
        mhr_path = mesh_data["mhr_path"]
        if os.path.exists(mhr_path):
            return mhr_path

    # Strategy 2: Check environment variable
    env_path = os.environ.get("SAM3D_MHR_PATH", "")
    if env_path and os.path.exists(env_path):
        return env_path

    # Strategy 3: Search ComfyUI models/sam3dbody/ folder
    sam3dbody_dir = os.path.join(folder_paths.models_dir, "sam3dbody", "assets", "mhr_model.pt")
    if os.path.exists(sam3dbody_dir):
        return sam3dbody_dir

    # Strategy 4: Search HuggingFace cache
    hf_cache_base = os.path.expanduser("~/.cache/huggingface/hub/models--facebook--sam-3d-body-dinov3")
    if os.path.exists(hf_cache_base):
        pattern = os.path.join(hf_cache_base, "snapshots", "*", "assets", "mhr_model.pt")
        matches = glob.glob(pattern)
        if matches:
            return matches[0]

    return None


def load_joint_parents_from_mhr(mhr_path):
    """Load joint parents from MHR model file."""
    if not mhr_path or not os.path.exists(mhr_path):
        return None
    try:
        mhr_model = torch.jit.load(mhr_path, map_location='cpu')
        joint_parents = mhr_model.character_torch.skeleton.joint_parents
        return joint_parents.cpu().numpy()
    except Exception:
        return None


class SAM3DBodyProcess:
    """
    Performs 3D human mesh reconstruction from a single image.

    Takes an input image and outputs 3D mesh data including vertices, faces,
    pose parameters, and camera parameters.
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("SAM3D_MODEL", {
                    "tooltip": "Loaded SAM 3D Body model from Load node"
                }),
                "image": ("IMAGE", {
                    "tooltip": "Input image containing human subject"
                }),
                "bbox_threshold": ("FLOAT", {
                    "default": 0.8,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Confidence threshold for human detection bounding boxes"
                }),
                "inference_type": (["full", "body", "hand"], {
                    "default": "full",
                    "tooltip": "full: body+hand decoders, body: body decoder only, hand: hand decoder only"
                }),
            },
            "optional": {
                "mask": ("MASK", {
                    "tooltip": "Optional segmentation mask to guide reconstruction"
                }),
            }
        }

    RETURN_TYPES = ("SAM3D_OUTPUT", "SKELETON", "IMAGE")
    RETURN_NAMES = ("mesh_data", "skeleton", "debug_image")
    FUNCTION = "process"
    CATEGORY = "SAM3DBody/processing"

    def _compute_bbox_from_mask(self, mask):
        """Compute bounding box from binary mask."""
        rows = np.any(mask > 0.5, axis=1)
        cols = np.any(mask > 0.5, axis=0)

        if not rows.any() or not cols.any():
            return None

        rmin, rmax = np.where(rows)[0][[0, -1]]
        cmin, cmax = np.where(cols)[0][[0, -1]]

        return np.array([[cmin, rmin, cmax, rmax]], dtype=np.float32)

    def process(self, model, image, bbox_threshold=0.8, inference_type="full", mask=None):
        """Process image and reconstruct 3D human mesh."""

        from sam_3d_body import SAM3DBodyEstimator

        # Extract model components
        sam_3d_model = model["model"]
        model_cfg = model["model_cfg"]

        # Create estimator
        estimator = SAM3DBodyEstimator(
            sam_3d_body_model=sam_3d_model,
            model_cfg=model_cfg,
            human_detector=None,
            human_segmentor=None,
            fov_estimator=None,
        )

        # Convert ComfyUI image to numpy (BGR format for OpenCV)
        img_bgr = comfy_image_to_numpy(image)

        # Convert mask if provided and compute bounding box
        mask_np = None
        bboxes = None
        if mask is not None:
            mask_np = comfy_mask_to_numpy(mask)
            if mask_np.ndim == 3:
                mask_np = mask_np[0]
            bboxes = self._compute_bbox_from_mask(mask_np)

        # Save image to temporary file (required by SAM3DBodyEstimator)
        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp:
            cv2.imwrite(tmp.name, img_bgr)
            tmp_path = tmp.name

        try:
            outputs = estimator.process_one_image(
                tmp_path,
                bboxes=bboxes,
                masks=mask_np,
                bbox_thr=bbox_threshold,
                use_mask=(mask is not None),
                inference_type=inference_type,
            )
        finally:
            if os.path.exists(tmp_path):
                os.unlink(tmp_path)

        if not outputs or len(outputs) == 0:
            raise RuntimeError("No people detected in image")

        # Take the first person if multiple detected
        output = outputs[0]

        # Extract mesh data
        mesh_data = {
            "vertices": output.get("pred_vertices", None),
            "faces": estimator.faces,
            "joints": output.get("pred_keypoints_3d", None),
            "joint_coords": output.get("pred_joint_coords", None),
            "joint_rotations": output.get("pred_global_rots", None),
            "camera": output.get("pred_cam_t", None),
            "focal_length": output.get("focal_length", None),
            "bbox": output.get("bbox", None),
            "pose_params": {
                "body_pose": output.get("body_pose_params", None),
                "hand_pose": output.get("hand_pose_params", None),
                "global_rot": output.get("global_rot", None),
                "shape": output.get("shape_params", None),
                "scale": output.get("scale_params", None),
                "expr": output.get("expr_params", None),
            },
            "raw_output": output,
            "all_people": outputs,
            "mhr_path": model.get("mhr_path", None),
        }

        # Extract skeleton data
        skeleton = {
            "joint_positions": output.get("pred_joint_coords", None),
            "joint_rotations": output.get("pred_global_rots", None),
            "pose_params": output.get("body_pose_params", None),
            "shape_params": output.get("shape_params", None),
            "scale_params": output.get("scale_params", None),
            "hand_pose": output.get("hand_pose_params", None),
            "global_rot": output.get("global_rot", None),
            "expr_params": output.get("expr_params", None),
            "camera": output.get("pred_cam_t", None),
            "focal_length": output.get("focal_length", None),
        }

        # Add joint parent hierarchy from the MHR model
        joint_parents = None

        # Try in-memory model first
        try:
            if hasattr(sam_3d_model, 'mhr_head') and hasattr(sam_3d_model.mhr_head, 'mhr'):
                mhr = sam_3d_model.mhr_head.mhr
                if hasattr(mhr, 'character_torch') and hasattr(mhr.character_torch, 'skeleton'):
                    skeleton_obj = mhr.character_torch.skeleton
                    if hasattr(skeleton_obj, 'joint_parents'):
                        parent_tensor = skeleton_obj.joint_parents
                        if isinstance(parent_tensor, torch.Tensor):
                            joint_parents = parent_tensor.cpu().numpy()
        except Exception:
            pass

        # Fallback: load from MHR model file
        if joint_parents is None:
            mhr_path = model.get("mhr_path") or find_mhr_model_path(mesh_data)
            joint_parents = load_joint_parents_from_mhr(mhr_path)

        if joint_parents is not None:
            skeleton["joint_parents"] = joint_parents

        # Create debug visualization
        from ..base import numpy_to_comfy_image
        debug_img = self._create_debug_visualization(img_bgr, outputs, estimator.faces)
        debug_img_comfy = numpy_to_comfy_image(debug_img)

        return (mesh_data, skeleton, debug_img_comfy)

    def _create_debug_visualization(self, img_bgr, outputs, faces):
        """Create a debug visualization of the results."""
        # Return original image for now
        return img_bgr


class SAM3DBodyProcessAdvanced:
    """
    Advanced processing node with full control over detection, segmentation, and FOV estimation.
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("SAM3D_MODEL", {
                    "tooltip": "Loaded SAM 3D Body model from Load node"
                }),
                "image": ("IMAGE", {
                    "tooltip": "Input image containing human subject"
                }),
                "bbox_threshold": ("FLOAT", {
                    "default": 0.8,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Confidence threshold for human detection"
                }),
                "nms_threshold": ("FLOAT", {
                    "default": 0.3,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Non-maximum suppression threshold for detection"
                }),
                "inference_type": (["full", "body", "hand"], {
                    "default": "full",
                    "tooltip": "Inference mode: full (body+hand), body only, or hand only"
                }),
                "detector_name": (["none", "vitdet"], {
                    "default": "none",
                    "tooltip": "Human detector to use (requires detector_path)"
                }),
                "segmentor_name": (["none", "sam2"], {
                    "default": "none",
                    "tooltip": "Segmentation model to use (requires segmentor_path)"
                }),
                "fov_name": (["none", "moge2"], {
                    "default": "none",
                    "tooltip": "FOV estimator to use (requires fov_path)"
                }),
            },
            "optional": {
                "detector_path": ("STRING", {
                    "default": "",
                    "tooltip": "Path to detector model or set SAM3D_DETECTOR_PATH env var"
                }),
                "segmentor_path": ("STRING", {
                    "default": "",
                    "tooltip": "Path to segmentor model or set SAM3D_SEGMENTOR_PATH env var"
                }),
                "fov_path": ("STRING", {
                    "default": "",
                    "tooltip": "Path to FOV model or set SAM3D_FOV_PATH env var"
                }),
                "mask": ("MASK", {
                    "tooltip": "Optional pre-computed segmentation mask"
                }),
            }
        }

    RETURN_TYPES = ("SAM3D_OUTPUT", "SKELETON", "IMAGE")
    RETURN_NAMES = ("mesh_data", "skeleton", "debug_image")
    FUNCTION = "process_advanced"
    CATEGORY = "SAM3DBody/advanced"

    def _compute_bbox_from_mask(self, mask):
        """Compute bounding box from binary mask."""
        rows = np.any(mask > 0.5, axis=1)
        cols = np.any(mask > 0.5, axis=0)

        if not rows.any() or not cols.any():
            return None

        rmin, rmax = np.where(rows)[0][[0, -1]]
        cmin, cmax = np.where(cols)[0][[0, -1]]

        return np.array([[cmin, rmin, cmax, rmax]], dtype=np.float32)

    def process_advanced(self, model, image, bbox_threshold=0.8, nms_threshold=0.3,
                        inference_type="full", detector_name="none", segmentor_name="none",
                        fov_name="none", detector_path="", segmentor_path="", fov_path="", mask=None):
        """Process image with advanced options."""

        from sam_3d_body import SAM3DBodyEstimator

        # Extract model components
        sam_3d_model = model["model"]
        model_cfg = model["model_cfg"]
        device = torch.device(model["device"])

        # Initialize optional components
        detector = None
        segmentor = None
        fov_estimator = None

        # Load detector if specified
        if detector_name != "none":
            detector_path = detector_path or os.environ.get("SAM3D_DETECTOR_PATH", "")
            if detector_path:
                from tools.build_detector import HumanDetector
                detector = HumanDetector(name=detector_name, device=device, path=detector_path)

        # Load segmentor if specified
        if segmentor_name != "none":
            segmentor_path = segmentor_path or os.environ.get("SAM3D_SEGMENTOR_PATH", "")
            if segmentor_path:
                from tools.build_sam import HumanSegmentor
                segmentor = HumanSegmentor(name=segmentor_name, device=device, path=segmentor_path)

        # Load FOV estimator if specified
        if fov_name != "none":
            fov_path = fov_path or os.environ.get("SAM3D_FOV_PATH", "")
            if fov_path:
                from tools.build_fov_estimator import FOVEstimator
                fov_estimator = FOVEstimator(name=fov_name, device=device, path=fov_path)

        # Create estimator with optional components
        estimator = SAM3DBodyEstimator(
            sam_3d_body_model=sam_3d_model,
            model_cfg=model_cfg,
            human_detector=detector,
            human_segmentor=segmentor,
            fov_estimator=fov_estimator,
        )

        # Convert image and mask
        img_bgr = comfy_image_to_numpy(image)
        mask_np = None
        bboxes = None
        if mask is not None:
            mask_np = comfy_mask_to_numpy(mask)
            if mask_np.ndim == 3:
                mask_np = mask_np[0]
            bboxes = self._compute_bbox_from_mask(mask_np)

        # Save to temp file
        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp:
            cv2.imwrite(tmp.name, img_bgr)
            tmp_path = tmp.name

        try:
            outputs = estimator.process_one_image(
                tmp_path,
                bboxes=bboxes,
                masks=mask_np,
                bbox_thr=bbox_threshold,
                nms_thr=nms_threshold,
                use_mask=(mask is not None or segmentor is not None),
                inference_type=inference_type,
            )
        finally:
            if os.path.exists(tmp_path):
                os.unlink(tmp_path)

        if not outputs or len(outputs) == 0:
            raise RuntimeError("No people detected in image")

        # Take the first person if multiple detected
        output = outputs[0]

        # Extract mesh data
        mesh_data = {
            "vertices": output.get("pred_vertices", None),
            "faces": estimator.faces,
            "joints": output.get("pred_keypoints_3d", None),
            "joint_coords": output.get("pred_joint_coords", None),
            "joint_rotations": output.get("pred_global_rots", None),
            "camera": output.get("pred_cam_t", None),
            "focal_length": output.get("focal_length", None),
            "bbox": output.get("bbox", None),
            "pose_params": {
                "body_pose": output.get("body_pose_params", None),
                "hand_pose": output.get("hand_pose_params", None),
                "global_rot": output.get("global_rot", None),
                "shape": output.get("shape_params", None),
                "scale": output.get("scale_params", None),
                "expr": output.get("expr_params", None),
            },
            "raw_output": output,
            "all_people": outputs,
        }

        # Extract skeleton data
        skeleton = {
            "joint_positions": output.get("pred_joint_coords", None),
            "joint_rotations": output.get("pred_global_rots", None),
            "pose_params": output.get("body_pose_params", None),
            "shape_params": output.get("shape_params", None),
            "scale_params": output.get("scale_params", None),
            "hand_pose": output.get("hand_pose_params", None),
            "global_rot": output.get("global_rot", None),
            "expr_params": output.get("expr_params", None),
            "camera": output.get("pred_cam_t", None),
            "focal_length": output.get("focal_length", None),
        }

        # Add joint parent hierarchy from the MHR model
        joint_parents = None

        # Try in-memory model first
        try:
            if hasattr(sam_3d_model, 'mhr_head') and hasattr(sam_3d_model.mhr_head, 'mhr'):
                mhr = sam_3d_model.mhr_head.mhr
                if hasattr(mhr, 'character_torch') and hasattr(mhr.character_torch, 'skeleton'):
                    skeleton_obj = mhr.character_torch.skeleton
                    if hasattr(skeleton_obj, 'joint_parents'):
                        parent_tensor = skeleton_obj.joint_parents
                        if isinstance(parent_tensor, torch.Tensor):
                            joint_parents = parent_tensor.cpu().numpy()
        except Exception:
            pass

        # Fallback: load from MHR model file
        if joint_parents is None:
            mhr_path = find_mhr_model_path()
            joint_parents = load_joint_parents_from_mhr(mhr_path)

        if joint_parents is not None:
            skeleton["joint_parents"] = joint_parents

        # Create debug visualization
        from ..base import numpy_to_comfy_image
        debug_img = self._create_debug_visualization(img_bgr, outputs, estimator.faces)
        debug_img_comfy = numpy_to_comfy_image(debug_img)

        return (mesh_data, skeleton, debug_img_comfy)

    def _create_debug_visualization(self, img_bgr, outputs, faces):
        """Create debug visualization."""
        return img_bgr


# Register nodes
NODE_CLASS_MAPPINGS = {
    "SAM3DBodyProcess": SAM3DBodyProcess,
    "SAM3DBodyProcessAdvanced": SAM3DBodyProcessAdvanced,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "SAM3DBodyProcess": "SAM 3D Body: Process Image",
    "SAM3DBodyProcessAdvanced": "SAM 3D Body: Process Image (Advanced)",
}
